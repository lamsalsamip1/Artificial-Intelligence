{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR Gate Realization using Backpropagation Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the neural network\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,layers,alpha=0.1):\n",
    "\n",
    "        self.W=[] # Initialize weights\n",
    "        self.layers=layers\n",
    "        self.alpha=alpha\n",
    "    \n",
    "        #Set weithgs to a random value\n",
    "        for i in np.arange(0,len(layers)-2):\n",
    "\n",
    "            w=np.random.randn(layers[i]+1,layers[i+1]+1)\n",
    "            self.W.append(w/np.sqrt(layers[i]))\n",
    "\n",
    "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "        self.W.append(w / np.sqrt(layers[-2]))\n",
    "\t\n",
    "    # construct and return a string that represents the network\n",
    "    def __repr__(self):\n",
    "\n",
    "        return \"NeuralNetwork: {}\".format(\n",
    "\t\t\t\"-\".join(str(l) for l in self.layers))               \n",
    "\t\t\n",
    "    #Sigmoid function\n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def sigmoid_deriv(self,x):\n",
    "        return x*(1-x)\n",
    "\n",
    "    \n",
    "    #Training function\n",
    "    def fit(self,X,y,epochs=1000,displayUpdate=100):\n",
    "\n",
    "        #insert a column of 1's as the last entry in the feature matrix\n",
    "        #this trick allows us to treat the bias as a trainable parameter\n",
    "        X=np.c_[X,np.ones((X.shape[0]))]\n",
    "\n",
    "        #loop over the desired number of epochs\n",
    "        for epoch in np.arange(0,epochs):\n",
    "\n",
    "            #loop over individual data point and train\n",
    "            for (x,target) in zip(X,y):\n",
    "                self.fit_partial(x,target)\n",
    "            \n",
    "            #check to see if we should display a training update\n",
    "            if epoch == 0 or (epoch + 1) % displayUpdate == 0: \n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(\"[INFO] epoch={}, loss={:.7f}\".format(epoch + 1, loss))\n",
    "\n",
    "\n",
    "    #backpropagation implementation\n",
    "    def fit_partial(self,x,y):\n",
    "        #construct a list of output activations for each layer\n",
    "\n",
    "        A = [np.atleast_2d(x)]\n",
    "\n",
    "        #Forward propagate\n",
    "        for layer in np.arange(0,len(self.W)):\n",
    "\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "            out= self.sigmoid(net)\n",
    "\n",
    "            A.append(out)\n",
    "\n",
    "        #Back propagation\n",
    "        \n",
    "        #compute error\n",
    "        error=A[-1]-y\n",
    "        D=[error*self.sigmoid_deriv(A[-1])]\n",
    "\n",
    "        #Now implement chain rule\n",
    "        for layer in np.arange(len(A)-2,0,-1):\n",
    "\n",
    "            delta= D[-1].dot(self.W[layer].T)\n",
    "            delta = delta * self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    "        \n",
    "        D = D[::-1] # reverse as we loop backwards\n",
    "\n",
    "        #Update weights\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
    "\n",
    "    #Prediction function\n",
    "    def predict(self, X, addBias=True):\n",
    "\n",
    "        p = np.atleast_2d(X)\n",
    "\n",
    "        if addBias:\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "        \n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "        \n",
    "        return p\n",
    "    \n",
    "    #Loss function\n",
    "    def calculate_loss(self, X, targets):\n",
    "        # make predictions for the input data points then compute\n",
    "        # the loss\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "        # return the loss\n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Gate Realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#truth table\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch=1, loss=0.5037561\n",
      "[INFO] epoch=100, loss=0.4990359\n",
      "[INFO] epoch=200, loss=0.4968438\n",
      "[INFO] epoch=300, loss=0.4926766\n",
      "[INFO] epoch=400, loss=0.4806505\n",
      "[INFO] epoch=500, loss=0.4462782\n",
      "[INFO] epoch=600, loss=0.3689504\n",
      "[INFO] epoch=700, loss=0.2155669\n",
      "[INFO] epoch=800, loss=0.0880848\n",
      "[INFO] epoch=900, loss=0.0448995\n",
      "[INFO] epoch=1000, loss=0.0282782\n",
      "[INFO] epoch=1100, loss=0.0201201\n",
      "[INFO] epoch=1200, loss=0.0154178\n",
      "[INFO] epoch=1300, loss=0.0124058\n",
      "[INFO] epoch=1400, loss=0.0103301\n",
      "[INFO] epoch=1500, loss=0.0088216\n",
      "[INFO] epoch=1600, loss=0.0076803\n",
      "[INFO] epoch=1700, loss=0.0067891\n",
      "[INFO] epoch=1800, loss=0.0060754\n",
      "[INFO] epoch=1900, loss=0.0054921\n",
      "[INFO] epoch=2000, loss=0.0050069\n",
      "[INFO] epoch=2100, loss=0.0045975\n",
      "[INFO] epoch=2200, loss=0.0042476\n",
      "[INFO] epoch=2300, loss=0.0039455\n",
      "[INFO] epoch=2400, loss=0.0036820\n",
      "[INFO] epoch=2500, loss=0.0034504\n",
      "[INFO] epoch=2600, loss=0.0032453\n",
      "[INFO] epoch=2700, loss=0.0030624\n",
      "[INFO] epoch=2800, loss=0.0028984\n",
      "[INFO] epoch=2900, loss=0.0027506\n",
      "[INFO] epoch=3000, loss=0.0026166\n",
      "[INFO] epoch=3100, loss=0.0024947\n",
      "[INFO] epoch=3200, loss=0.0023833\n",
      "[INFO] epoch=3300, loss=0.0022812\n",
      "[INFO] epoch=3400, loss=0.0021872\n",
      "[INFO] epoch=3500, loss=0.0021004\n",
      "[INFO] epoch=3600, loss=0.0020201\n",
      "[INFO] epoch=3700, loss=0.0019455\n",
      "[INFO] epoch=3800, loss=0.0018761\n",
      "[INFO] epoch=3900, loss=0.0018113\n",
      "[INFO] epoch=4000, loss=0.0017508\n",
      "[INFO] epoch=4100, loss=0.0016940\n",
      "[INFO] epoch=4200, loss=0.0016407\n",
      "[INFO] epoch=4300, loss=0.0015906\n",
      "[INFO] epoch=4400, loss=0.0015434\n",
      "[INFO] epoch=4500, loss=0.0014988\n",
      "[INFO] epoch=4600, loss=0.0014567\n",
      "[INFO] epoch=4700, loss=0.0014168\n",
      "[INFO] epoch=4800, loss=0.0013790\n",
      "[INFO] epoch=4900, loss=0.0013431\n",
      "[INFO] epoch=5000, loss=0.0013090\n",
      "[INFO] epoch=5100, loss=0.0012765\n",
      "[INFO] epoch=5200, loss=0.0012456\n",
      "[INFO] epoch=5300, loss=0.0012161\n",
      "[INFO] epoch=5400, loss=0.0011879\n",
      "[INFO] epoch=5500, loss=0.0011610\n",
      "[INFO] epoch=5600, loss=0.0011352\n",
      "[INFO] epoch=5700, loss=0.0011106\n",
      "[INFO] epoch=5800, loss=0.0010869\n",
      "[INFO] epoch=5900, loss=0.0010642\n",
      "[INFO] epoch=6000, loss=0.0010425\n",
      "[INFO] epoch=6100, loss=0.0010216\n",
      "[INFO] epoch=6200, loss=0.0010014\n",
      "[INFO] epoch=6300, loss=0.0009821\n",
      "[INFO] epoch=6400, loss=0.0009635\n",
      "[INFO] epoch=6500, loss=0.0009455\n",
      "[INFO] epoch=6600, loss=0.0009282\n",
      "[INFO] epoch=6700, loss=0.0009115\n",
      "[INFO] epoch=6800, loss=0.0008954\n",
      "[INFO] epoch=6900, loss=0.0008798\n",
      "[INFO] epoch=7000, loss=0.0008647\n",
      "[INFO] epoch=7100, loss=0.0008502\n",
      "[INFO] epoch=7200, loss=0.0008361\n",
      "[INFO] epoch=7300, loss=0.0008224\n",
      "[INFO] epoch=7400, loss=0.0008092\n",
      "[INFO] epoch=7500, loss=0.0007964\n",
      "[INFO] epoch=7600, loss=0.0007840\n",
      "[INFO] epoch=7700, loss=0.0007720\n",
      "[INFO] epoch=7800, loss=0.0007603\n",
      "[INFO] epoch=7900, loss=0.0007490\n",
      "[INFO] epoch=8000, loss=0.0007380\n",
      "[INFO] epoch=8100, loss=0.0007273\n",
      "[INFO] epoch=8200, loss=0.0007169\n",
      "[INFO] epoch=8300, loss=0.0007068\n",
      "[INFO] epoch=8400, loss=0.0006969\n",
      "[INFO] epoch=8500, loss=0.0006874\n",
      "[INFO] epoch=8600, loss=0.0006780\n",
      "[INFO] epoch=8700, loss=0.0006690\n",
      "[INFO] epoch=8800, loss=0.0006601\n",
      "[INFO] epoch=8900, loss=0.0006515\n",
      "[INFO] epoch=9000, loss=0.0006431\n",
      "[INFO] epoch=9100, loss=0.0006349\n",
      "[INFO] epoch=9200, loss=0.0006270\n",
      "[INFO] epoch=9300, loss=0.0006192\n",
      "[INFO] epoch=9400, loss=0.0006116\n",
      "[INFO] epoch=9500, loss=0.0006042\n",
      "[INFO] epoch=9600, loss=0.0005969\n",
      "[INFO] epoch=9700, loss=0.0005898\n",
      "[INFO] epoch=9800, loss=0.0005829\n",
      "[INFO] epoch=9900, loss=0.0005762\n",
      "[INFO] epoch=10000, loss=0.0005696\n",
      "[INFO] epoch=10100, loss=0.0005631\n",
      "[INFO] epoch=10200, loss=0.0005568\n",
      "[INFO] epoch=10300, loss=0.0005506\n",
      "[INFO] epoch=10400, loss=0.0005446\n",
      "[INFO] epoch=10500, loss=0.0005387\n",
      "[INFO] epoch=10600, loss=0.0005329\n",
      "[INFO] epoch=10700, loss=0.0005272\n",
      "[INFO] epoch=10800, loss=0.0005217\n",
      "[INFO] epoch=10900, loss=0.0005162\n",
      "[INFO] epoch=11000, loss=0.0005109\n",
      "[INFO] epoch=11100, loss=0.0005057\n",
      "[INFO] epoch=11200, loss=0.0005006\n",
      "[INFO] epoch=11300, loss=0.0004955\n",
      "[INFO] epoch=11400, loss=0.0004906\n",
      "[INFO] epoch=11500, loss=0.0004858\n",
      "[INFO] epoch=11600, loss=0.0004811\n",
      "[INFO] epoch=11700, loss=0.0004764\n",
      "[INFO] epoch=11800, loss=0.0004719\n",
      "[INFO] epoch=11900, loss=0.0004674\n",
      "[INFO] epoch=12000, loss=0.0004630\n",
      "[INFO] epoch=12100, loss=0.0004587\n",
      "[INFO] epoch=12200, loss=0.0004545\n",
      "[INFO] epoch=12300, loss=0.0004503\n",
      "[INFO] epoch=12400, loss=0.0004463\n",
      "[INFO] epoch=12500, loss=0.0004422\n",
      "[INFO] epoch=12600, loss=0.0004383\n",
      "[INFO] epoch=12700, loss=0.0004344\n",
      "[INFO] epoch=12800, loss=0.0004306\n",
      "[INFO] epoch=12900, loss=0.0004269\n",
      "[INFO] epoch=13000, loss=0.0004232\n",
      "[INFO] epoch=13100, loss=0.0004196\n",
      "[INFO] epoch=13200, loss=0.0004161\n",
      "[INFO] epoch=13300, loss=0.0004126\n",
      "[INFO] epoch=13400, loss=0.0004091\n",
      "[INFO] epoch=13500, loss=0.0004058\n",
      "[INFO] epoch=13600, loss=0.0004024\n",
      "[INFO] epoch=13700, loss=0.0003992\n",
      "[INFO] epoch=13800, loss=0.0003959\n",
      "[INFO] epoch=13900, loss=0.0003928\n",
      "[INFO] epoch=14000, loss=0.0003897\n",
      "[INFO] epoch=14100, loss=0.0003866\n",
      "[INFO] epoch=14200, loss=0.0003836\n",
      "[INFO] epoch=14300, loss=0.0003806\n",
      "[INFO] epoch=14400, loss=0.0003777\n",
      "[INFO] epoch=14500, loss=0.0003748\n",
      "[INFO] epoch=14600, loss=0.0003719\n",
      "[INFO] epoch=14700, loss=0.0003691\n",
      "[INFO] epoch=14800, loss=0.0003664\n",
      "[INFO] epoch=14900, loss=0.0003636\n",
      "[INFO] epoch=15000, loss=0.0003610\n",
      "[INFO] epoch=15100, loss=0.0003583\n",
      "[INFO] epoch=15200, loss=0.0003557\n",
      "[INFO] epoch=15300, loss=0.0003531\n",
      "[INFO] epoch=15400, loss=0.0003506\n",
      "[INFO] epoch=15500, loss=0.0003481\n",
      "[INFO] epoch=15600, loss=0.0003457\n",
      "[INFO] epoch=15700, loss=0.0003432\n",
      "[INFO] epoch=15800, loss=0.0003408\n",
      "[INFO] epoch=15900, loss=0.0003385\n",
      "[INFO] epoch=16000, loss=0.0003361\n",
      "[INFO] epoch=16100, loss=0.0003338\n",
      "[INFO] epoch=16200, loss=0.0003316\n",
      "[INFO] epoch=16300, loss=0.0003293\n",
      "[INFO] epoch=16400, loss=0.0003271\n",
      "[INFO] epoch=16500, loss=0.0003250\n",
      "[INFO] epoch=16600, loss=0.0003228\n",
      "[INFO] epoch=16700, loss=0.0003207\n",
      "[INFO] epoch=16800, loss=0.0003186\n",
      "[INFO] epoch=16900, loss=0.0003165\n",
      "[INFO] epoch=17000, loss=0.0003145\n",
      "[INFO] epoch=17100, loss=0.0003125\n",
      "[INFO] epoch=17200, loss=0.0003105\n",
      "[INFO] epoch=17300, loss=0.0003085\n",
      "[INFO] epoch=17400, loss=0.0003066\n",
      "[INFO] epoch=17500, loss=0.0003046\n",
      "[INFO] epoch=17600, loss=0.0003027\n",
      "[INFO] epoch=17700, loss=0.0003009\n",
      "[INFO] epoch=17800, loss=0.0002990\n",
      "[INFO] epoch=17900, loss=0.0002972\n",
      "[INFO] epoch=18000, loss=0.0002954\n",
      "[INFO] epoch=18100, loss=0.0002936\n",
      "[INFO] epoch=18200, loss=0.0002919\n",
      "[INFO] epoch=18300, loss=0.0002901\n",
      "[INFO] epoch=18400, loss=0.0002884\n",
      "[INFO] epoch=18500, loss=0.0002867\n",
      "[INFO] epoch=18600, loss=0.0002850\n",
      "[INFO] epoch=18700, loss=0.0002833\n",
      "[INFO] epoch=18800, loss=0.0002817\n",
      "[INFO] epoch=18900, loss=0.0002801\n",
      "[INFO] epoch=19000, loss=0.0002785\n",
      "[INFO] epoch=19100, loss=0.0002769\n",
      "[INFO] epoch=19200, loss=0.0002753\n",
      "[INFO] epoch=19300, loss=0.0002738\n",
      "[INFO] epoch=19400, loss=0.0002722\n",
      "[INFO] epoch=19500, loss=0.0002707\n",
      "[INFO] epoch=19600, loss=0.0002692\n",
      "[INFO] epoch=19700, loss=0.0002677\n",
      "[INFO] epoch=19800, loss=0.0002663\n",
      "[INFO] epoch=19900, loss=0.0002648\n",
      "[INFO] epoch=20000, loss=0.0002634\n"
     ]
    }
   ],
   "source": [
    "#define neural network and train it\n",
    "nn = NeuralNetwork([2, 2, 1], alpha=0.5)\n",
    "nn.fit(X, y, epochs=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data=[0 0], ground-truth=0, pred=0.0130, step=0\n",
      "[INFO] data=[0 1], ground-truth=1, pred=0.9869, step=1\n",
      "[INFO] data=[1 0], ground-truth=1, pred=0.9890, step=1\n",
      "[INFO] data=[1 1], ground-truth=0, pred=0.0082, step=0\n"
     ]
    }
   ],
   "source": [
    "# Run prediction\n",
    "for (x, target) in zip(X, y):\n",
    "\t# make a prediction on the data point and display the result\n",
    "\t# to our console\n",
    "\tpred = nn.predict(x)[0][0]\n",
    "\tstep = 1 if pred > 0.5 else 0\n",
    "\tprint(\"[INFO] data={}, ground-truth={}, pred={:.4f}, step={}\".format(\n",
    "\t\tx, target[0], pred, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
